{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Logistic Regression\n",
        "Bu çalışmada lojistik regresyon hakkında bilgi edineceğiz. Somut olarak, tweet'lerde duyarlılık analizi için lojistik regresyon uygulayacağız. Bir tweet verildiğinde, olumlu mu yoksa olumsuz mu olduğuna karar vereceğiz. Özellikle şunları yapacağız:\n",
        "\n",
        "* Bazı metinler verildiğinde lojistik regresyon için özelliklerin nasıl çıkarılacağını öğrenme\n",
        "* Lojistik regresyonu sıfırdan uygulama\n",
        "* Doğal bir dil işleme görevinde lojistik regresyon uygulama\n",
        "* Lojistik regresyonumuzu kullanarak test etme\n",
        "* Hata analizi yapma\n",
        "\n",
        "Bir tweet veri seti kullanacağız. Paketlere yüklemek için aşağıdaki hücreyi çalıştıralım."
      ],
      "metadata": {
        "id": "b3R4NwCv2WZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Functions and Data"
      ],
      "metadata": {
        "id": "GUb5tR3P3ZHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnFBlyby2La2",
        "outputId": "cb4fa932-f0ef-4cb8-b582-0e71472184e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "import w1_unittest\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imported Functions\n",
        "\n",
        "Bu çalışma için gerekli verileri indirelim. [Twitter_samples veri kümesi için belgelere](http://www.nltk.org/howto/twitter.html) adresinden ulaşabiliriz.\n",
        "\n",
        "* twitter_samples: Bu not defterini yerel bilgisayarımızda çalıştırıyorsak, aşağıdakileri kullanarak indirmemiz gerekir:\n",
        "```Python\n",
        "nltk.download('twitter_samples')\n",
        "```\n",
        "\n",
        "* stopwords: Bu not defterini yerel bilgisayarımızda çalıştırıyorsak, aşağıdakileri kullanarak indirmemiz gerekir:\n",
        "```python\n",
        "nltk.download('stopwords')\n",
        "```\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* process_tweet: process_tweet: metni temizler, ayrı sözcüklere ayırır, stopwords sözcükleri kaldırır ve sözcükleri köklere dönüştürür.\n",
        "* build_freqs: Bu işlem, 'korpus' adı verilen (tweet setinin tamamı) bir metinde belirli bir kelimenin pozitif etiket '1' veya negatif etiket '0' ile ne sıklıkta ilişkilendirildiğini sayar. Ardından, 'freqs' adlı bir sözlük oluşturulur. Burada her anahtar bir (kelime, etiket) demetidir ve değer, tweetler külliyatındaki o kelimenin belirli etiketle ne sıklıkta görüldüğünün sayısıdır."
      ],
      "metadata": {
        "id": "BXLWbITY3xXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "D_1rKJPy3hmg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "from utils import process_tweet, build_freqs"
      ],
      "metadata": {
        "id": "-Rrkt3bG4wzB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "* `twitter_samples`  beş bin olumlu tweet, beş bin olumsuz tweet ve tam seti olan 10,000 tweetin alt kümelerini içerir.\n",
        "    * Eğer üç veri kümesini de kullanırsak, olumlu tweetlerin ve olumsuz tweetlerin tekrarlarını tanımlamış oluruz.\n",
        "    * adece beş bin olumlu tweet ve beş bin olumsuz tweet seçeceğiz."
      ],
      "metadata": {
        "id": "OlRqoAJu5D2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "cBVB0QFd4yyo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train test split (Eğitim ve test verisi ayırma): Verilerin %20'si test setine, %80'i ise eğitim setine dahil edilecek."
      ],
      "metadata": {
        "id": "NlnI-cCb5iK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "G3sRavur5c8Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Olumlu etiketlerin ve olumsuz etiketlerin NumPy dizilerini oluşturalım."
      ],
      "metadata": {
        "id": "ejVDJew35xRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "ItRnVM8h5qiw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLSu2RvE52DQ",
        "outputId": "9045dd40-7c1a-474c-f0ea-82e74efb4c6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* İçe aktarılan build_freqs fonksiyonunu kullanarak frekans sözlüğünü oluşturalım.\n",
        "\n",
        "```Python\n",
        "    for y,tweet in zip(ys, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "```\n",
        "* Dikkat edelim, dıştaki for döngüsü her bir tweeti gezinir ve içteki for döngüsü her bir tweetteki kelimeyi gezinir.\n",
        "* 'freqs' sözlüğü, oluşturulan frekans sözlüğüdür.\n",
        "* Anahtar, (\"happy\", 1) veya (\"happy\", 0) gibi demet olan (kelime, etiket) çiftidir. Her bir anahtar için depolanan değer, \"happy\" kelimesinin pozitif bir etiketle ilişkilendirilme sayısı veya negatif bir etiketle ilişkilendirilme sayısıdır."
      ],
      "metadata": {
        "id": "ca8skP1N55dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiEFs_in53gY",
        "outputId": "1470c528-1d0f-4649-e627-8fade94bf3c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Tweet\n",
        "Verilen 'process_tweet' fonksiyonu tweet'i ayrıştırarak (tokenize) kelimelere ayırır, stop kelimelerini kaldırır ve stemming işlemi uygular."
      ],
      "metadata": {
        "id": "9CuKktAx6lLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ08gg_f6f1A",
        "outputId": "3229c1ac-432c-4153-94d4-fc1d78fe8bbf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Logistic Regression\n",
        "\n",
        "\n",
        "### 1.1 - Sigmoid\n",
        "Metin sınıflandırması için lojistik regresyonu kullanmayı öğreneceğiz\n",
        "* Sigmoid fonksiyonu aşağıdaki gibi tanımlanır:\n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "Bu fonksiyon, girdiyi 'z' değerini 0 ile 1 arasında değerlere eşler ve bu nedenle olasılık olarak ele alınabilir.\n",
        "\n",
        "![Ekran görüntüsü 2023-07-22 232757.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "s7DZCWq0Rz7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 -  sigmoid\n",
        "Sigmoid fonksiyonunu uygulayalım.\n",
        "\n",
        "* Bu fonksiyonu, z hem bir sayı (scalar) olduğunda hem de bir dizi (array) olduğunda çalışacak şekilde oluşturmalıyız."
      ],
      "metadata": {
        "id": "8uMRTAZgS07l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
        "\n",
        "</ul>\n",
        "</p>"
      ],
      "metadata": {
        "id": "P-7GUUhjTGas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # calculate the sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "U5yuJe_e6tn4"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing your function\n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('SUCCESS!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('CORRECT!')\n",
        "else:\n",
        "    print('Oops again!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmDZc3tfTNIu",
        "outputId": "64ccefff-a8d8-4028-80a6-d8793e01a623"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "CORRECT!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest.test_sigmoid(sigmoid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLjpu2WETPDN",
        "outputId": "fbab9b42-2518-4f03-8301-b7a44f979737"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression: Regression and a Sigmoid\n",
        "\n",
        "Lojistik regresyon, düzenli bir lineer regresyonu alır ve lineer regresyonun çıktısına bir sigmoid uygular.\n",
        "\n",
        "Lineer Regresyon:\n",
        "\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "\n",
        "Burada $\\theta$ değerleri \"ağırlıklar\" olarak adlandırılır. Eğer derin öğrenme uzmanlık dersini aldıysanız, ağırlıklara 'w' vektörü ile atıfta bulunmuş olabilirsiniz. Bu kurs kapsamında, ağırlıklara farklı bir değişken olan $\\theta$ ile atıfta bulunuyoruz.\n",
        "\n",
        "Lojistik Regresyon:\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "\n",
        "Burada 'z', 'logit' olarak adlandırılır."
      ],
      "metadata": {
        "id": "Ke8pVbtNTfTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Cost function and Gradient\n",
        "\n",
        "Lojistik regresyon için kullanılan maliyet fonksiyonu, tüm eğitim örnekleri için log kaybının ortalamasıdır:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ eğitim örneklerinin sayısıdır.\n",
        "* $y^{(i)}$  eğitim örneğinin gerçek etiketidir.\n",
        "* $h(z^{(i)})$  modelin eğitim örneği 'i' için tahminidir.\n",
        "\n",
        "The loss function for a single training example is (Tek bir eğitim örneği için kayıp fonksiyonu) :\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* Tüm $h$ değerleri 0 ile 1 arasında olduğundan, logaritmalar negatif olacaktır. Bu nedenle, iki kayıp teriminin toplamına uygulanan -1 faktörüdür.\n",
        "* Model 1'i ($h(z(\\theta)) = 1$) tahmin ettiğinde ve etiket 'y' da 1 olduğunda, o eğitim örneği için kayıp 0 olur.\n",
        "* Benzer şekilde, model 0'ı ($h(z(\\theta)) = 0$) tahmin ederken ve gerçek etiket de 0 olduğunda, o eğitim örneği için kayıp 0 olur.\n",
        "* Ancak model tahmini 1'e yaklaştığında ($h(z(\\theta)) = 0.9999$) ve etiket 0 olduğunda, log kaybının ikinci terimi büyük bir negatif sayı olur ve daha sonra -1 faktörü ile çarpılarak pozitif bir kayıp değerine dönüştürülür. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ Model tahmini 1'e ne kadar yaklaşırsa, kayıp o kadar büyük olur."
      ],
      "metadata": {
        "id": "ocqCKyoiUPtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
        "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYcvJFdRTRRe",
        "outputId": "20011d80-e9d8-485e-840b-d620de26ac75"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976294"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Benzer şekilde, eğer model 0'a yakın bir değer tahmin eder ($h(z) = 0.0001$) ancak gerçek etiket 1 ise, kayıp fonksiyonunun ilk terimi büyük bir sayı olur: $-1 \\times log(0.0001) \\approx 9.2$. Tahmin 0'a ne kadar yakınsa, kayıp o kadar büyük olur."
      ],
      "metadata": {
        "id": "9uEeVnbtVFP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
        "-1 * np.log(0.0001) # loss is about 9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUndtxHVVBWW",
        "outputId": "edaf5f2a-2a36-4f3f-82e4-0d24d604499f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.210340371976182"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update the weights\n",
        "\n",
        "Ağırlık vektörümüzü $\\theta$ güncellemek için, modelimizin tahminlerini iteratif olarak geliştirmek için gradyan descenti kullanacağız.\n",
        "\n",
        "Maliyet fonksiyonunun $J$ bir ağırlığa $\\theta_j$ göre gradyanı:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
        "* 'i', tüm 'm' eğitim örnekleri üzerindeki indekstir.\n",
        "* 'j', ağırlık $\\theta_j$ indeksidir, bu nedenle $x^{(i)}_j$, ağırlık $\\theta_j$ ile ilişkili özelliktir.\n",
        "\n",
        "* Ağırlık $\\theta_j$ güncellemek için, gradyanın $\\alpha$ ile belirlenen bir bölümünü çıkararak ayarlarız:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* Öğrenme hızı $\\alpha$, bir güncellemenin ne kadar büyük olacağını kontrol etmek için seçtiğimiz bir değerdir.\n"
      ],
      "metadata": {
        "id": "10DvjP6HVRqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 - gradientDescent\n",
        "Gradient descent fonksiyonunu uygulayalım.\n",
        "* 'num_iters' adlı değişken, tüm eğitim kümesini kaç kez kullanacağımızı belirler.\n",
        "* Her iterasyonda, tüm eğitim örnekleri (toplam 'm' eğitim örneği) ve tüm özellikler için maliyet fonksiyonunu hesaplayacağız.\n",
        "* Bir seferde tek bir ağırlığı $\\theta_i$ güncellemek yerine, sütun vektöründe tüm ağırlıkları güncelleyebiliriz:  \n",
        "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
        "\\theta_0\n",
        "\\\\\n",
        "\\theta_1\n",
        "\\\\\n",
        "\\theta_2\n",
        "\\\\\n",
        "\\vdots\n",
        "\\\\\n",
        "\\theta_n\n",
        "\\end{pmatrix}$$\n",
        "* $\\mathbf{\\theta}$, boyutları (n+1, 1) olan bir vektördür, burada 'n' özellik sayısıdır ve $\\theta_0$ için bir tane daha öğe vardır (ilgili özellik değeri $\\mathbf{x_0}$ not edin ki bu 1'dir).\n",
        "* 'Logit'ler, 'z', özellik matrisi 'x' ile ağırlık vektörü 'theta'yi çarparak hesaplanır. $z = \\mathbf{x}\\mathbf{\\theta}$\n",
        "    * $\\mathbf{x}$ boyutları (m, n+1)\n",
        "    * $\\mathbf{\\theta}$ boyutları (n+1, 1)\n",
        "    * $\\mathbf{z}$ boyutları (m, 1)\n",
        "* Tahmin 'h', 'z' içindeki her bir öğeye sigmoid uygulayarak hesaplanır: $h(z) = sigmoid(z)$, ve boyutları (m, 1)'dir.\n",
        "* Maliyet fonksiyonu $J$, 'y' ve 'log(h)' vektörlerinin dot ürünü alınarak hesaplanır. 'y' ve 'h' her ikisi de sütun vektörleridir (m,1), dolayısıyla matris çarpımı yapmak için vektörü sola transpoze edin ve böylece bir satır vektörün bir sütun vektör ile dot ürününü gerçekleştirir.\n",
        "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
        "* Aynı şekilde, $\\mathbf{x}$ boyutları (m, n+1) ve hem $\\mathbf{h}$ hem de $\\mathbf{y}$ (m, 1) boyutlarındadır, bu nedenle $\\mathbf{x}$'i transpoze edip matris çarpımı yapmak için sola koymanız gerekir, bu da (n+1, 1) boyutunda bir sonuç elde ederiz:\n",
        "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
      ],
      "metadata": {
        "id": "F8LfhgspWBO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>Matrix çarpımı için numpy.dot kullanalım.</li>\n",
        "    <li>Kesirin -1/m ondalık bir değer olmasını sağlamak için pay veya payda (veya ikisini) ondalık bir değere dönüştürelim, örneğin float(1) olarak veya 1. şeklinde float sürümü için yazalım. </li>\n",
        "</ul>\n",
        "</p>"
      ],
      "metadata": {
        "id": "BlFRe8Z9XnA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  '''\n",
        "    Input:\n",
        "      x: matrix of features which is (m,n+1)\n",
        "      y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "      theta: weight vector of dimension (n+1,1)\n",
        "      alpha: learning rate\n",
        "      num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "      J: the final cost\n",
        "      theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "  '''\n",
        "  ### START CODE HERE ###\n",
        "  # get 'm', the number of rows in matrix x\n",
        "  m = x.shape[0]\n",
        "\n",
        "  for i in range(0, num_iters):\n",
        "\n",
        "    # get z, the dot product of x and theta\n",
        "    z = np.dot(x,theta)\n",
        "\n",
        "    # get the sigmoid of z\n",
        "    h = sigmoid(z)\n",
        "\n",
        "    # calculate the cost function\n",
        "    J = -1/m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))\n",
        "\n",
        "    # update the weights theta\n",
        "    theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "  J = float(J)\n",
        "\n",
        "  return J, theta"
      ],
      "metadata": {
        "id": "oUefadvtVL51"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the function\n",
        "# Construct a synthetic test case using numpy PRNG functions\n",
        "np.random.seed(1)\n",
        "# X input is 10 x 3 with ones for the bias terms\n",
        "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
        "\n",
        "# Y Labels are 10 x 1\n",
        "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
        "\n",
        "# Apply gradient descent\n",
        "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
        "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIgISiTnYGhl",
        "outputId": "174adbfa-c40b-4ea0-819f-7304b053d175"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.67094970.\n",
            "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest.test_gradientDescent(gradientDescent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGlViDeVYIIm",
        "outputId": "0addfd9b-4365-4149-d614-02084717f1db"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Extracting the Features\n",
        "\n",
        "* Verilen tweetlerin bir listesi için, özellikleri çıkaralım ve bunları bir matrise depolayalım. İki özellik çıkaracağız.\n",
        "    * İlk özellik, bir tweetteki olumlu kelimelerin sayısıdır.\n",
        "    * İkinci özellik, bir tweetteki olumsuz kelimelerin sayısıdır.\n",
        "* Daha sonra bu özellikler üzerinde lojistik regresyon sınıflandırıcımızı eğitelim.\n",
        "* Sınıflandırıcıyı bir doğrulama kümesinde test edelim.\n",
        "\n",
        "\n",
        "### Exercise 3 - extract_features\n",
        "extract_features fonksiyonunu uygulayalım.\n",
        "* Bu fonksiyon, tek bir tweet'i alır.\n",
        "* İçe aktarılan process_tweet fonksiyonunu kullanarak tweet'i işleyelim ve tweet kelimelerinin listesini kaydedelim.\n",
        "* İşlenmiş kelimeler listesindeki her kelime için:\n",
        "    * Her kelime için, 'freqs' sözlüğünde olumlu '1' etiketiyle ilişkilendirildiğinde o kelimenin sayısını kontrol edelim (Anahtar için (kelime, 1.0) kontrol edelim.)\n",
        "    * Aynı şekilde, kelimenin olumsuz etiket '0' ile ilişkilendirildiğinde sayısını kontrol edelim (Anahtar için (kelime, 0.0) kontrol edelim.)\n",
        "\n",
        "Not: Yukarıda verilen uygulama talimatlarında, olumlu veya olumsuz olma tahmini, özellik vektörüne bağlıdır."
      ],
      "metadata": {
        "id": "TH4sRLJQnrEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>Sözlükte (kelime, etiket) anahtarının bulunmadığı durumları ele aldığımızdan emin olalım. </li>\n",
        "    <li> Bir Python sözlüğünün 'get' işlevini kullanmayla ilgili ipuçları için web'de arama yapalım. İşte bir <a href=\"https://www.programiz.com/python-programming/methods/dictionary/get\" > örneği </a> </li>\n",
        "</ul>\n",
        "</p>"
      ],
      "metadata": {
        "id": "ORsWPW0JonYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C3 GRADED FUNCTION: extract_features\n",
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements for [bias, positive, negative] counts\n",
        "    x = np.zeros(3)\n",
        "\n",
        "    # bias term is set to 1\n",
        "    x[0] = 1\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[1] += freqs.get((word, 1.0),0)\n",
        "\n",
        "        # increment the word count for the negative label 0\n",
        "        x[2] += freqs.get((word, 0.0),0)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    x = x[None, :]  # adding batch dimension for further processing\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x"
      ],
      "metadata": {
        "id": "VMLBAv52nY9T"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your function\n",
        "# test 1\n",
        "# test on training data\n",
        "tmp1 = extract_features(train_x[0], freqs)\n",
        "print(tmp1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3beWKMDjqLVy",
        "outputId": "6801ad62-eea0-42b1-e4c4-c33bc7b50a85"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 3.133e+03 6.100e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5-3uwqcqPWK",
        "outputId": "bde1b914-07c1-4300-c300-cdfc13d545d6"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest.test_extract_features(extract_features, freqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6sOiK1GqRp7",
        "outputId": "d97a2d9e-47d9-41fa-db22-83cf5da634dd"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Training Our Model\n",
        "\n",
        "Modelimizi eğitmek için:\n",
        "* Tüm eğitim örnekleri için özellikleri bir X matrisinde istifleyelim.\n",
        "* Yukarıda uyguladığımız `gradientDescent` i çağıralım."
      ],
      "metadata": {
        "id": "qoiRH-urqYRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETYc-bm5qUNk",
        "outputId": "83670409-7aa4-427a-ad1a-4166e72f9ca2"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.22521264.\n",
            "The resulting vector of weights is [6e-08, 0.0005382, -0.0005583]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4'></a>\n",
        "## 4 -  Test our Logistic Regression\n",
        "\n",
        "Modelimizin daha önce görmediği bazı yeni girdiler üzerinde lojistik regresyon işlevimizi test etmenin zamanı geldi.\n",
        "\n",
        "### Exercise 4 - predict_tweet\n",
        "`predict_tweet` uygulayalım.\n",
        "Bir tweet'in olumlu mu yoksa olumsuz mu olduğunu tahmin edelim.\n",
        "\n",
        "* Verilen bir tweet'i işleyelim, ardından özelliklerini çıkaralım.\n",
        "* Logitleri elde etmek için modelin öğrenilen ağırlıklarını özelliklere uygulayalım.\n",
        "* Tahmini elde etmek için sigmoid'i logitlere uygulayalım (0 ile 1 arasında bir değer).\n",
        "\n",
        "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
      ],
      "metadata": {
        "id": "VEJ-nJsJqenR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = extract_features(tweet,freqs)\n",
        "\n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "EOhAx0qRqbh8"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8i5WYcEqrYZ",
        "outputId": "5239fe06-9129-4900-b0cd-6bb156b1f331"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.519275\n",
            "I am bad -> 0.494347\n",
            "this movie should have been great. -> 0.515980\n",
            "great -> 0.516065\n",
            "great great -> 0.532097\n",
            "great great great -> 0.548063\n",
            "great great great great -> 0.563930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNGJR1tqsJz",
        "outputId": "68e3c09d-896e-4db3-9caa-3d30a7d40802"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.83110764]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest.test_predict_tweet(predict_tweet, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7VLBE6ZquuL",
        "outputId": "3f9cfdb6-09a5-4461-d042-237ce887a325"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 -  Check the Performance using the Test Set\n",
        "Yukarıdaki eğitim setini kullanarak modelimizi eğittikten sonra, modelimizi test setiyle test ederek gerçek, görünmeyen veriler üzerinde nasıl performans gösterebileceğini kontrol edelim.\n",
        "\n",
        "\n",
        "### Exercise 5 - test_logistic_regression\n",
        "`test_logistic_regression` u uygulama.\n",
        "* Eğitilen modelimizin test verileri ve ağırlıkları göz önüne alındığında, lojistik regresyon modelimizin doğruluğunu hesaplayalım.\n",
        "* Test setindeki her tweet için tahminde bulunmak için 'predict_tweet' işlevimizi kullanalım.\n",
        "* Tahmin > 0,5 ise, modelin 'y_hat' sınıflandırmasını 1 olarak ayarlayalım, aksi halde modelin 'y_hat' sınıflandırmasını 0 olarak ayarlayalım.\n",
        "* Bir tahmin, y_hat, test_y'ye eşit olduğunda doğrudur. Eşit olduklarında tüm örnekleri toplayalım ve m'ye bölelim.\n"
      ],
      "metadata": {
        "id": "K9Re2jP5qyZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>    \n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>Bir listeyi bir numpy dizisine dönüştürmek için np.asarray() kullanalım.</li>\n",
        "    <li>Bir (m,1) boyutlu diziyi bir (m,) dizisine dönüştürmek için numpy.squeeze() işlevini kullanalım. </li>\n",
        "</ul>\n",
        "</p>"
      ],
      "metadata": {
        "id": "olIY6klSq0Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5 GRADED FUNCTION: test_logistic_regression\n",
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "\n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "\n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "R4vfc9jaqwYT"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qjResUvq8WA",
        "outputId": "517757c6-0866-48c0-e20b-fe449d9fe6e1"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w1_unittest.unittest_test_logistic_regression(test_logistic_regression, freqs, theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtAINRXDq9Gr",
        "outputId": "f1965d49-e32b-4903-cf07-c24948d560b8"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 -  Error Analysis\n",
        "\n",
        "Bu bölümde, modelimizin yanlış sınıflandırdığı bazı tweet'leri göreceğiz."
      ],
      "metadata": {
        "id": "cxuErqw-rA_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some error analysis done for you\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6IXlx5xq_Y0",
        "outputId": "dfe15513-bfa6-4531-e62a-835bac1ed6fd"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48942981\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
            "http://t.co/UGQzOx0huu\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418981\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418981\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48418981\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49636406\tb'park get sunlight'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48250522\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50988296\tb'u prob fun david'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50040366\tb'pat jay'\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000002\tb'belov grandmoth'\n",
            "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
            "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
            "0\t0.50648699\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='6'></a>\n",
        "## 6 - Predict with your own Tweet"
      ],
      "metadata": {
        "id": "rBK3sDSerHUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the tweet below\n",
        "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "    print('Positive sentiment')\n",
        "else:\n",
        "    print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIr5PDJArDQ7",
        "outputId": "ee87e6e0-65ac-4740-a3df-2c5a36b46c69"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48125421]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTTL2ACxrJV0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}